#################################
#           메  모              #
#################################


runtime and Unified functions
eager execution

@tf.function 안의 코드도 이 효과로 쓰여진 순서대로 실행

In tf 1.x was used sesstion.run(), but in 2.0, tf.function() will be used.


"""차이점"""
# 텐서플로 1.x
outputs = session.run(f(placeholder), feed_dict={placeholder: input})
# 텐서플로 2.0
outputs = f(input)

텐서플로 1.x의 일반적인 사용 패턴은 키친 싱크 전략 입니다. 먼저 모든 연산을 결합하여 준비한 다음 session.run()을 사용해 선택한 텐서를 평가합니다.

텐서플로 2.0에서는 필요할 때 호출할 수 있는 작은 함수로 코드를 리팩토링 해야 합니다. 모델 훈련의 한단계나 정방향 연산 같은 고수준 연산에만 tf.function 데코레이터를 적용하세요.


기본 버전
def dense(x, W, b):
  return tf.nn.sigmoid(tf.matmul(x, W) + b)

@tf.function
def multilayer_perceptron(x, w0, b0, w1, b1, w2, b2 ...):
  x = dense(x, w0, b0)
  x = dense(x, w1, b1)
  x = dense(x, w2, b2)
  ...

# 여전히 w_i, b_i 변수를 직접 관리해야 합니다. 이 코드와 떨어져서 크기가 정의됩니다.


케라스 버전
# 각 층은 linear(x)처럼 호출 가능합니다.
layers = [tf.keras.layers.Dense(hidden_size, activation=tf.nn.sigmoid) for _ in range(n)]
perceptron = tf.keras.Sequential(layers)

# layers[3].trainable_variables => returns [w3, b3]
# perceptron.trainable_variables => returns [w0, b0, ...]


## example ##
전이 학습을 통한 예제
몸통(trunk)을 공유하는 다중 출력 모델 훈련

trunk = tf.keras.Sequential([...])
head1 = tf.keras.Sequential([...])
head2 = tf.keras.Sequential([...])

path1 = tf.keras.Sequential([trunk, head1])
path2 = tf.keras.Sequential([trunk, head2])

# 주된 데이터셋에서 훈련합니다.
for x, y in main_dataset:
  with tf.GradientTape() as tape:
    prediction = path1(x) -> x is a list of dataset in python.
    loss = loss_fn_head1(prediction, y)

  # trunk와 head1 가중치를 동시에 최적화합니다. (요 다음 과정에 대한 설명)
  gradients = tape.gradient(loss, path1.trainable_variables)
  optimizer.apply_gradients(zip(gradients, path1.trainable_variables))

# trunk를 재사용하여 head2를 세부 튜닝합니다.
for x, y in small_dataset:
  with tf.GradientTape() as tape:
    prediction = path2(x)
    loss = loss_fn_head2(prediction, y)
  # trunk 가중치는 제외하고 head2 가중치만 최적화합니다.
  gradients = tape.gradient(loss, head2.trainable_variables)
  optimizer.apply_gradients(zip(gradients, head2.trainable_variables))

# trunk 연산만 재사용을 위해 저장할 수 있습니다.
tf.saved_model.save(trunk, output_path)















